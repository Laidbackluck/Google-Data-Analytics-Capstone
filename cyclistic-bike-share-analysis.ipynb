{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The Scenario","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"I’ve been enrolled in the Google Data Analytics Professional Certificate since July and am currently finishing up the last course.  For the last course, they recommend working on some case studies in which we can add to our portfolios.  They gave us a couple of options to work on or we could also find and work on a case study of our own choosing.  I chose to work with the first case study they provided, How Does a Bike-Share Navigate Speedy Success?.\n\nFor this case study, I am working as a junior data analyst in the marketing team for Cyclistic, a bike-sharing company located in Chicago.  The goal of this case study is to help Cyclistic create a marketing campaign in order to maximize the number of annual memberships.  We will do so by answering the following three questions:\nHow do annual members and casual riders use Cyclistic bikes differently?\nWhy would casual riders buy Cyclistic annual memberships?\nHow can Cyclistic use digital media to influence casual riders to become members?\nIn doing so, my task is to find trends and gain insights that will help the Cyclistic executive team decide on the recommended marketing program.\n","metadata":{}},{"cell_type":"markdown","source":"# Data Sources ","metadata":{}},{"cell_type":"markdown","source":"The data used for analysis is from [HERE](https://divvy-tripdata.s3.amazonaws.com/index.html). The data used in the previous 12 months of Cyclistic trip data from September 2020 - August 2021.  The data is from a reliable source, Motivate International Inc under this [license](https://www.divvybikes.com/data-license-agreement) and is organized in 12 separate “.csv” files, one for each month.  Each file contains 13 attributes per observation, containing rider IDs, which location they picked up the bike to when they got off, and so on. This data will be used to find trends between casual riders and annual members. ","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning ","metadata":{}},{"cell_type":"markdown","source":"For the cleaning process, my goal is to combine all the files into one, with a consistent format throughout the file, in terms of data structures and naming conventions and so forth. I will start the process by loading all the files into a SQL database, so I can organize, sort, filter, and clean the data.  I chose to use BigQuery from the Google Cloud Platform since that is what I’ve been using throughout the course. At a quick glance of the data, I noticed that there are a bunch of nulls in various attributes that will need further inspection to see if they can be filled in as much as possible.  \nAfter loading all the files into BigQuery, I wrote a quick query to join all the files into one.  At first, my query using UNION ALL did not run.  So upon further inspection, it seems that Station IDs started changing formats around December 2020, where instead of just integers (ex. 639), some had letters in front (ex. TA1306000029).  I decided to not include the Station IDs since we have just about the same information from the Station Names. I saved this query into another table called “all_data” to use for further data cleaning. [QUERY](https://console.cloud.google.com/bigquery?sq=198905033215:a0688ca410d3477d9d44a3b89acfe67c)\n\n```\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202009\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202010\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202011\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202012\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202101\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202102\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202103\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202104\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202105\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202106\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202107\nUNION ALL\nSELECT\n  * EXCEPT ( start_station_id,\n    end_station_id )\nFROM\n  case_study_1.trip_data_202108 AS all_data\n  ```\nNow that I have all the data in one combined file, what I did next was format the Station Names since I noticed they were inconsistent ([QUERY](https://console.cloud.google.com/bigquery?sq=198905033215:7fac197573c949e68a6adfdba8d2e192)). \n```\nSELECT\n  ride_id,\n  TRIM(REPLACE(start_station_name, '(*)', '')) start_station_name,\n  TRIM(REPLACE(end_station_name, '(*)', '')) end_station_name\nFROM (\n  SELECT\n    ride_id,\n    TRIM(REPLACE(start_station_name, '(Temp)', '')) start_station_name,\n    TRIM(REPLACE(end_station_name, '(Temp)', '')) end_station_name\n  FROM\n    case_study_1.all_data)\n    \n```\nI also changed the latitude and longitude coordinates to match with the station names ([QUERY](https://console.cloud.google.com/bigquery?sq=198905033215:e533ad9a7a794835a84cd7c8a0d1bea4)).\n```\nSELECT \n    DISTINCT station_name,\n    ROUND(AVG(lat), 6) AS lat,\n    ROUND(AVG(lng), 6) AS lng,\nFROM (\n    SELECT \n        DISTINCT TRIM(REPLACE(start_station_name, '(*)', '')) AS station_name, \n        AVG(lat) AS lat,\n        AVG(lng) AS lng, \n    FROM (\n        SELECT \n            DISTINCT TRIM(REPLACE(start_station_name, '(Temp)', '')) AS start_station_name,\n            AVG(start_lat) AS lat,\n            AVG(start_lng) AS lng\n        FROM\n            case_study_1.all_data\n        GROUP BY 1) AS stat_start\n    GROUP BY \n        1\n    UNION DISTINCT \n    SELECT \n        DISTINCT TRIM(REPlACE(end_station_name, '(*)', '')) AS station_name,\n        AVG(lat) AS lat,\n        AVG(lng) AS lng\n    FROM (\n        SELECT \n            DISTINCT TRIM(REPLACE(end_station_name, '(Temp)', '')) AS end_station_name,\n            AVG(end_lat) AS lat,\n            AVG(end_lng) AS lng,\n        FROM \n            case_study_1.all_data\n        GROUP BY 1) AS stat_end\n    GROUP BY \n        1)\nGROUP BY \n    1\n```\nAnd lastly, I added two new columns for the day of the week as well as the duration of the trip in minutes ([QUERY](https://console.cloud.google.com/bigquery?sq=198905033215:2a574751461c4f35b99d21324d8749b4)).\n```\nSELECT\n    *,\n    TIMESTAMP_DIFF(ended_at, started_at, MINUTE) AS ride_length_min,\n    CASE\n        WHEN EXTRACT (DAYOFWEEK FROM started_at) = 1 THEN 'Sunday'\n        WHEN EXTRACT (DAYOFWEEK FROM started_at) = 2 THEN 'Monday'\n        WHEN EXTRACT (DAYOFWEEK FROM started_at) = 3 THEN 'Tuesday'\n        WHEN EXTRACT (DAYOFWEEK FROM started_at) = 4 THEN 'Wednesday'\n        WHEN EXTRACT (DAYOFWEEK FROM started_at) = 5 THEN 'Thursday'\n        WHEN EXTRACT (DAYOFWEEK FROM started_at) = 6 THEN 'Friday'\n        ELSE \n            'Staturday'\n    END \n        AS day_of_week\nFROM\n    case_study_1.all_data \n```\nWe can use this aggregate data later in the analysis phase.  Upon further inspection, I noticed there were some negative integers under the new column for the duration of the trip.  It appears that the data was entered incorrectly for about ~1200 entries out of the 4.9 million so I deleted these entries from the dataset ([QUERY](https://console.cloud.google.com/bigquery?sq=198905033215:423e874a395148c3af24098926e1f298)).\n```\nDELETE \n\nFROM \n    case_study_1.clean_data\nWHERE \n    ride_length_min < 0\n```\nNow that the data is processed and cleaned, I saved the new file under “clean_data” so I can use this file for analysis ([QUERY](https://console.cloud.google.com/bigquery?sq=198905033215:a4cccdaa3a7f4da7916dd795d2f54aac)).\n```\nWITH \n    clean_data_name AS (\n        SELECT\n            agg_data.ride_id,\n            agg_data.rideable_type,\n            agg_data.member_casual,\n            agg_data.started_at,\n            agg_data.ended_at,\n            agg_data.day_of_week,\n            agg_data.ride_length_min,\n            stat_name.start_station_name,\n            agg_data.start_lat,\n            agg_data.start_lng,\n            stat_name.end_station_name,\n            agg_data.end_lat,\n            agg_data.end_lng\n        FROM \n            case_study_1.aggregate_data AS agg_data \n        LEFT JOIN \n            case_study_1.clean_station_name AS stat_name\n        ON \n            agg_data.ride_id = stat_name.ride_id),\n    clean_data_start AS(\n        SELECT\n            clean_agg.ride_id,\n            clean_agg.rideable_type,\n            clean_agg.member_casual,\n            clean_agg.started_at,\n            clean_agg.ended_at,\n            clean_agg.day_of_week,\n            clean_agg.ride_length_min,\n            clean_agg.start_station_name,\n            clean_lat_lng.lat AS start_lat,\n            clean_lat_lng.lng AS start_lng,\n            clean_agg.end_station_name,\n            clean_agg.end_lat,\n            clean_agg.end_lng\n        FROM \n            clean_data_name AS clean_agg\n        LEFT JOIN \n            case_study_1.clean_lat_lng AS clean_lat_lng\n        ON \n            clean_agg.start_station_name = clean_lat_lng.station_name),\n    clean_data AS(\n        SELECT \n            clean_start.ride_id,\n            clean_start.rideable_type,\n            clean_start.member_casual,\n            clean_start.started_at,\n            clean_start.ended_at,\n            clean_start.day_of_week,\n            clean_start.ride_length_min,\n            clean_start.start_station_name,\n            clean_start.start_lat,\n            clean_start.start_lng,\n            clean_start.end_station_name,\n            clean_lat_lng.lat AS end_lat,\n            clean_lat_lng.lng AS end_lng\n        FROM \n            clean_data_start AS clean_start\n        LEFT JOIN \n            case_study_1.clean_lat_lng AS clean_lat_lng\n        ON \n            clean_start.end_station_name = clean_lat_lng.station_name)\nSELECT \n    *\nFROM\n    clean_data\n```","metadata":{}},{"cell_type":"markdown","source":"# Analysis","metadata":{}},{"cell_type":"markdown","source":"Now that we have a clean and processed .csv file, I will first upload the file into R to do some initial analysis and see if there are any trends we can find. ","metadata":{}},{"cell_type":"code","source":"library(tidyverse)\nlibrary(skimr)\n\ndf <- read.csv(\"~/R/Case_Study_1/clean_data.csv\")\n\n# Quick summary of data\nskim_without_charts(df)\n\n# General summary of avg ride duration & longest ride\nsummarize(df, \n          \"Avg Ride Duration (Min)\" = mean(ride_length_min),\n          \"Longest Ride (Min)\" = max(ride_length_min)) \n\n# General avg ride duration & longest ride by day\ndf %>% group_by(day_of_week) %>% \n  summarize(\"Avg Ride Duration (Min)\" = mean(ride_length_min),\n            \"Longest Ride (Min)\" = max(ride_length_min))\n\n# General summary of busiest & slowest day of the week\ndf %>% count(\"Busiest Day\" = day_of_week) %>% slice(which.max(n)) \ndf %>% count(\"Slowest Day\" = day_of_week) %>% slice(which.min(n)) \n\n# Number of casuals vs members\ndf %>% count(\"status\" = member_casual)\n\n# Summary stats between casual & members - avg ride duration & longest ride\ndf %>% group_by(\"status\" = member_casual) %>% \n  summarize(\"Avg Ride Duration (Min)\" = mean(ride_length_min),\n            \"Longest Ride (Min)\" = max(ride_length_min))\n\n# Summary stats between casual & members by day of week - avg ride duration and longest ride\ndf %>% group_by(\"status\" = member_casual, day_of_week) %>% \n  summarize(\"Avg Ride Duration (Min)\" = mean(ride_length_min),\n            \"Longest Ride (Min)\" = max(ride_length_min))\n\n# Summary stats between casual & members - busiest & slowest day\ndf %>% group_by(\"status\" = member_casual) %>% count(\"Busiest Day\" = day_of_week) %>% slice(which.max(n)) \ndf %>% group_by(\"status\" = member_casual) %>% count(\"Slowest Day\" = day_of_week) %>% slice(which.min(n))\n\n\n# Summary stats between casual & members by type of bike - avg duration & longest ride\ndf %>% group_by(\"status\" = member_casual, \"Type of Bike\" = rideable_type) %>%\n  summarize(\"Avg Ride Duration (Min)\" = mean(ride_length_min),\n            \"Longest Ride (Min)\" = max(ride_length_min))\n\n# Summary stats between casual & members by type of bike - busiest and slowest days\ndf %>% group_by(\"status\" = member_casual, \"Type of Bike\" = rideable_type) %>% \n  count(\"Busiest Day\" = day_of_week) %>% slice(which.max(n))\ndf %>% group_by(\"status\" = member_casual, \"Type of Bike\" = rideable_type) %>% \n  count(\"Slowest Day\" = day_of_week) %>% slice(which.min(n))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After about 50 lines of code, here are my initial findings:\n\n* For both casual and members, the average ride time is around 23 mins, with the busiest days on the weekends.  \n* However, for only casual riders, they have an average ride time of 33 mins, where as members are only around 14 mins. \n* The busiest day for casual riders was saturday and slowest day was wednesday, where as the busiest day was wednesday and slowest day was sunday for members.\n* Casual members also rode with docked bikes the longest for an average of 59 mins, almost as twice as long compared to classic or electric bikes, 29 mins and 20 mins respectively.\n* Member riders has and average trip of around 13 - 15 mins per type of bike.\n\nAlthough we got some nice insights from the exploratory analysis from R, I will load the into [Tableau](https://public.tableau.com/app/profile/laidbackluck/viz/CaseStudy-CyclisticBike-ShareAnalysis/YearTrend) as well to see if we can gain further insights by creating visuals for analysis.","metadata":{}},{"cell_type":"markdown","source":"### Tableau Findings","metadata":{}},{"cell_type":"markdown","source":"\nFrom our first visual, we can see that riders have the most trips around mid summer to early autum, around the months of early may to late september.\n\nAs we compare the number of trips of the different types of bikes, for both members and casual riders, the most rode type of bike is the classic, followed by electric and docked respectively. \n\nAs we look at the number of trips by day and as well as each hour, we can see that casual riders tent to have more trips on the weekends, Saturday and Sunday, and also around the afternoon hours.  \nAs for the member riders, they tend to have the most trips on the weekdays, Monday through Friday, and have spikes around early morning (7 & 8 AM) and the evening (5 & 6 PM), indicating members are more likely to be using the bikes to travel to and from work. \n\nAs we visualize the initial findings of ride duration from our analysis from R, we can see that casual riders tend to ride for longer on the weekend, whereas members tend to stay roughly around the same throughout the week.  \nFor hours of the day however, there are some spikes throughout the day for casual riders, whereas members also stay roughly around the same.  There is a noticeably huge spike for both casual and members around 2 - 4 AM that will need further analysis. \n\nFinally as we look at the number of trips by station.  We can see the busiest stations tend to be around the Harbor and Gold Coast.","metadata":{}},{"cell_type":"markdown","source":"# Conclusions & Recommendations","metadata":{}},{"cell_type":"markdown","source":"From the analysis, we can see a trend of having more riders around the summer time.  We can also assume that current members tend to be riding to and from work mostly on weekday; and casual riders are more frequent on the weekends.   Also casual riders tend to ride for longer where as current members ride for a rough average of around 15 mins.  \n\nIt would be most advantageous to run a marketing campaign around summertime and on the weekends to capture the most casual riders in hopes of converting them to members.  Ideally the best locations to target would be around the harbor and Gold Coast since those tend to have the busiest stations.  We can also capitalize on the tendency of casual riders riding for longer durations by incentivizing a loyalty program, where they can get rewards based on how long they ride.","metadata":{}}]}